# -*- coding: utf-8 -*-
"""sentiment_analysisRF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AnYlSuiZuNXCNBLLOPR-kQ5lkCpTK_K0
"""

import pandas as pd
import numpy as np

#train data
data=pd.read_csv("train.csv")
data.head()

"""preprocessing"""

data.shape

data.value_counts('label')

import seaborn as sns

sns.countplot(x='label',hue = "label",data=data)

#test data
df=pd.read_csv("test.csv")
df.head()

"""for train data"""

data['tweet'] = data['tweet'].str.replace('@user',' ')
data['tweet'] = data['tweet'].str.replace('#',' ')
data['tweet'] = data['tweet'].str.replace("[^0-9a-zA-Z#]",' ')
data

"""for test data"""

df['tweet'] = df['tweet'].str.replace('@user',' ')
df['tweet'] = df['tweet'].str.replace('#',' ')
df['tweet'] = df['tweet'].str.replace("[^a-zA-Z#]",' ')
df

from collections import Counter

all_words=[]
for line in list(data['tweet']):
  words=line.split()
  for word in words:
    all_words.append(word.lower())

a=Counter(all_words).most_common(5)

a

data['tweet']=data['tweet'].apply(lambda x:x.split())
data.head()

"""for test"""

all_words1=[]
for line in list(df['tweet']):
  words=line.split()
  for word in words:
    all_words1.append(word.lower())

a=Counter(all_words1).most_common(5)

a

df['tweet']=df['tweet'].apply(lambda x:x.split())
df.head()

"""Stemming

"""

from nltk import PorterStemmer
stemmer=PorterStemmer()
data['tweet']=data['tweet'].apply(lambda x: [stemmer.stem(i) for i in x])
data.head()

from nltk import PorterStemmer
stemmer=PorterStemmer()
df['tweet']=df['tweet'].apply(lambda x: [stemmer.stem(i) for i in x ])
df.head()

"""stopwords"""

import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
stopwords=nltk.corpus.stopwords.words('english')

newStopWords=['u','go','got','via','or','ur','us','in','in','i','let','the','to','is','amp','make','one','day','days','get']
stopwords.extend(newStopWords)

import string
def process(text):
  nopunc=set(char for char in list(text) if char not in string.punctuation)
  nonpunc= " ".join(nopunc)
  return[word for word in nonpunc.lower().split() if word.lower() not in stopwords]

data['tweet']=data['tweet'].apply(process)

data.head()

"""for test"""

df['tweet']=df['tweet'].apply(process)

df.head()

from wordcloud import WordCloud
import matplotlib.pyplot as plt
word=[]
for line in data['tweet']:
  word.extend(line)
wordfreq=Counter(word)
wordcloud=WordCloud(background_color='white',max_words=2000,stopwords=stopwords).generate_from_frequencies(wordfreq)
plt.figure(figsize=(10,9))
plt.imshow(wordcloud,interpolation='bilinear')
plt.axis("off")
plt.show()

from wordcloud import WordCloud
import matplotlib.pyplot as plt
word1=[]
for line in df['tweet']:
  word1.extend(line)
wordfreq=Counter(word1)
wordcloud=WordCloud(background_color='white',max_words=2000,stopwords=stopwords).generate_from_frequencies(wordfreq)
plt.figure(figsize=(10,9))
plt.imshow(wordcloud,interpolation='bilinear')
plt.axis("off")
plt.show()

def string (text):
  to_return=" "
  for i in list(text):
    to_return += str(i) + " "
  to_return = to_return[:-1]

  return to_return

data['tweet']=data['tweet'].apply(string)
data.head()

df['tweet']=df['tweet'].apply(string)
df.head()

positive=[r for r in data['tweet'][data['label']==0]]
pos=''.join(positive)

wordcloud=WordCloud(background_color='white',max_words=2000,stopwords=stopwords).generate(pos)
plt.figure(figsize=(10,9))
plt.imshow(wordcloud,interpolation='bilinear')
plt.axis("off")
plt.show()

negative=[r for r in data['tweet'][data['label']==1]]
neg=''.join(negative)

wordcloud=WordCloud(background_color='white',max_words=2000,stopwords=stopwords).generate(neg)
plt.figure(figsize=(10,9))
plt.imshow(wordcloud,interpolation='bilinear')
plt.axis("off")
plt.show()

#data=data.drop(["id"],axis=1)
#df=df.drop(["id"],axis=1)

data.head()
#train data

df.head()
#test data

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test=train_test_split(data['tweet'],data['label'],random_state=42,test_size=0.2)
print("training set:",x_train.shape,y_train.shape)
print("testing set:",x_test.shape,y_test.shape)

from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer
count_vect=CountVectorizer(stop_words='english')
transformer=TfidfTransformer(norm='l2',sublinear_tf=True)

test_x=data['tweet']
test_x

from nltk.parse.corenlp import transform
x_train_counts=count_vect.fit_transform(x_train)
x_train_tfidf=transformer.fit_transform(x_train_counts)
print(x_train_counts.shape)
print(x_train_tfidf.shape)

test_x_counts=count_vect.transform(test_x)
test_x_tfidf=transformer.transform(test_x_counts)
print(test_x_counts.shape)
print(test_x_tfidf.shape)

x_test_counts=count_vect.transform(x_test)
x_test_tfidf=transformer.transform(x_test_counts)
print(x_test_counts.shape)
print(x_test_tfidf.shape)

from sklearn.ensemble import RandomForestClassifier
model=RandomForestClassifier(n_estimators=200)
model.fit(x_train_tfidf,y_train)

predictions=model.predict(x_test_tfidf)

submission=model.predict(test_x_tfidf)
submission

from sklearn.metrics import accuracy_score
accuracy_score(y_test,predictions)*100

from sklearn.metrics import confusion_matrix,f1_score
confusion_matrix(y_test,predictions)

